{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khlodMohamed/Computer-Vision/blob/main/Image%20segmentation/UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsV9zhqQIA6M"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkmyA2o6IDFu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "!pip install albumentations==0.4.6\n",
        "import albumentations \n",
        "\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAmD5FFqdpdB"
      },
      "source": [
        "##Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8pzfeVblLUh",
        "outputId": "11e97512-5361-43d5-9075-be770cca1d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connecting to drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3mv1AlNFbwQ"
      },
      "outputs": [],
      "source": [
        "os.mkdir('/content/zip') \n",
        "%cd /content/zip\n",
        "!wget https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Data.zip\n",
        "!wget https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Part1_GroundTruth.zip\n",
        "!wget https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Validation_Data.zip\n",
        "!wget https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Validation_Part1_GroundTruth.zip\n",
        "!wget https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Test_v2_Data.zip\n",
        "!wget https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Test_v2_Part1_GroundTruth.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLy-1DkQWO-5"
      },
      "outputs": [],
      "source": [
        "os.mkdir('/content/unzip') \n",
        "%cd /content/unzip\n",
        "print(os.getcwd())\n",
        "!unzip /content/zip/ISIC-2017_Training_Data.zip\n",
        "!unzip /content/zip/ISIC-2017_Training_Part1_GroundTruth.zip\n",
        "!unzip /content/zip/ISIC-2017_Validation_Data.zip\n",
        "!unzip /content/zip/ISIC-2017_Validation_Part1_GroundTruth.zip\n",
        "!unzip /content/zip/ISIC-2017_Test_v2_Data.zip\n",
        "!unzip /content/zip/ISIC-2017_Test_v2_Part1_GroundTruth.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MwbhvETcROW",
        "outputId": "38f68060-3f53-4ed3-e40b-30d6280ff995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/unzip/ISIC-2017_Training_Data\n",
            "/content/unzip/ISIC-2017_Validation_Data\n",
            "/content/unzip/ISIC-2017_Test_v2_Data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/unzip/ISIC-2017_Training_Data\n",
        "!rm ISIC-2017_Training_Data_metadata.csv\n",
        "!rm *.png\n",
        "%cd /content/unzip/ISIC-2017_Validation_Data\n",
        "!rm ISIC-2017_Validation_Data_metadata.csv\n",
        "!rm *.png\n",
        "%cd /content/unzip/ISIC-2017_Test_v2_Data\n",
        "!rm ISIC-2017_Test_v2_Data_metadata.csv\n",
        "!rm *.png\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsIfDwzGcbu6",
        "outputId": "a00b284e-b400-4b66-e97c-6821de32282b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/unzip/ISIC-2017_Training_Data\n",
            "/content/unzip/ISIC-2017_Validation_Data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "#remove .ipynb_checkpoints\n",
        "%cd /content/unzip/ISIC-2017_Training_Data\n",
        "%rm -rf `find -type d -name .ipynb_checkpoints`\n",
        "%cd /content/unzip/ISIC-2017_Validation_Data\n",
        "%rm -rf `find -type d -name .ipynb_checkpoints`\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_ZYBBPXtWS0",
        "outputId": "5421724f-5492-427b-a7d3-bb27ebabc40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "2000\n",
            "150\n",
            "150\n",
            "600\n",
            "600\n"
          ]
        }
      ],
      "source": [
        "\n",
        "val_ = os.listdir('/content/unzip/ISIC-2017_Validation_Data')\n",
        "val_m = os.listdir('/content/unzip/ISIC-2017_Validation_Part1_GroundTruth')\n",
        "train_ = os.listdir('/content/unzip/ISIC-2017_Training_Data')\n",
        "train_m = os.listdir('/content/unzip/ISIC-2017_Training_Part1_GroundTruth')\n",
        "test_ = os.listdir('/content/unzip/ISIC-2017_Test_v2_Data')\n",
        "test_m = os.listdir('/content/unzip/ISIC-2017_Test_v2_Part1_GroundTruth')\n",
        "print(len(train_))\n",
        "print(len(train_m))\n",
        "print(len(val_))\n",
        "print(len(val_m))\n",
        "print(len(test_))\n",
        "print(len(test_m))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKSHLGoddwEk"
      },
      "source": [
        "##Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdmmgvWfd2_F"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "def delete(folder_path):\n",
        "  for filename in os.listdir(folder_path):\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "      try:\n",
        "          if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "              os.unlink(file_path)\n",
        "          elif os.path.isdir(file_path):\n",
        "              shutil.rmtree(file_path)\n",
        "      except Exception as e:\n",
        "          print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "def show_img(folder_path, i):\n",
        "  temp = os.listdir(folder_path)\n",
        "  img_path = os.path.join(folder_path, temp[i])\n",
        "  img = cv2.imread(img_path)\n",
        "  print(img.shape)\n",
        "  cv2_imshow(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4S7NxTcZBpw"
      },
      "source": [
        "##Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5hvpFcZZMIj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class ISICDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "        # self.images = self.images[0:20]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_segmentation.png\"))\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wih1bMUqbabr"
      },
      "source": [
        "##Utili\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uznOuwxJbZt-",
        "outputId": "0cf09214-d9b7-4ee7-f975-74128b16501e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "%cd /content\n",
        "def save_checkpoint(state, filename=\"/content/drive/MyDrive/Colab Notebooks/Computer Vision/my_checkpoint2.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    test_dir,\n",
        "    test_maskdir,\n",
        "    batch_size_train,\n",
        "    batch_size_val,\n",
        "    batch_size_test,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    train_ds = ISICDataset(\n",
        "        image_dir=train_dir,\n",
        "        mask_dir=train_maskdir,\n",
        "        transform=train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size_train,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_ds = ISICDataset(\n",
        "        image_dir=val_dir,\n",
        "        mask_dir=val_maskdir,\n",
        "        transform=val_transform,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size_val,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    test_ds = ISICDataset(\n",
        "        image_dir=val_dir,\n",
        "        mask_dir=val_maskdir,\n",
        "        transform=val_transform,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size_test,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZF5JJBYdIiH"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mVVQgYPc6xX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d( \n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((3, 1, 161, 161))\n",
        "    model = UNET(in_channels=1, out_channels=1)\n",
        "    preds = model(x)\n",
        "    assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRPuyFsJH9KU"
      },
      "source": [
        "##Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Nlf2Z8H710"
      },
      "outputs": [],
      "source": [
        "\n",
        "def loss_fn(inputs, targets, smooth=0.5):\n",
        "\n",
        "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "    inputs = torch.sigmoid(inputs)    \n",
        "    inputs.data = (inputs.data > 0.5 ).float()\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "    #flatten label and prediction tensors\n",
        "\n",
        "    \n",
        "    #intersection is equivalent to True Positive count\n",
        "    #union is the mutually inclusive area of all labels & predictions \n",
        "    intersection = (inputs * targets).sum()\n",
        "    total = (inputs + targets).sum()\n",
        "    union = total - intersection \n",
        "    \n",
        "    IoU = (intersection + smooth)/(union + smooth)\n",
        "    return 1 - IoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8bFPoRbPsUD"
      },
      "source": [
        "##Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZANdeRaPp9h"
      },
      "outputs": [],
      "source": [
        "def get_transformations():\n",
        "  train_transform = A.Compose(\n",
        "      [\n",
        "          A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "          A.Rotate(limit=35, p=0.75),\n",
        "          # A.HorizontalFlip(p=0.5),\n",
        "          A.HorizontalFlip(p=0.75),\n",
        "          A.VerticalFlip(p=0.75),\n",
        "          A.Normalize(\n",
        "              mean=[0.0, 0.0, 0.0],\n",
        "              std=[1.0, 1.0, 1.0],\n",
        "              max_pixel_value=255.0,\n",
        "          ),\n",
        "          \n",
        "          ToTensorV2(),\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  val_transforms = A.Compose(\n",
        "      [\n",
        "          A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "          A.Normalize(\n",
        "              mean=[0.0, 0.0, 0.0],\n",
        "              std=[1.0, 1.0, 1.0],\n",
        "              max_pixel_value=255.0,\n",
        "          ),\n",
        "          ToTensorV2(),\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  return train_transform, val_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd8lWPCldMVp"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE8oY2duMi4h"
      },
      "outputs": [],
      "source": [
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    losses = []\n",
        "    avg_loss = 0\n",
        "    loop = tqdm(loader, total=len(loader))\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            losses.append(loss)\n",
        "            avg_loss = sum(losses)/len(losses)\n",
        "\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update tqdm loop\n",
        "        loop.set_description(f\"Train\")\n",
        "        loop.set_postfix(loss = avg_loss.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def val_fn(loader, model, loss_fn):\n",
        "    loop = tqdm(loader, total=len(loader))\n",
        "    avg_loss = 0\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, targets) in enumerate(loop):\n",
        "          data = data.to(device=DEVICE)\n",
        "          targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "          predictions = model(data)\n",
        "          loss = loss_fn(predictions, targets)\n",
        "          losses.append(loss)\n",
        "          avg_loss = sum(losses)/len(losses)\n",
        "          loop.set_description(f\"Val\")\n",
        "          loop.set_postfix(loss = avg_loss.cpu().numpy())\n",
        "\n",
        "    model.train()\n",
        "    return avg_loss\n",
        " \n",
        "def test_fn(loader, model, loss_fn, folder):\n",
        "  loop = tqdm(loader, total=len(loader))\n",
        "  losses = []\n",
        "  avg_loss = 0\n",
        "  model.eval()\n",
        "  for batch_idx, (data, targets) in enumerate(loop):\n",
        "    data = data.to(device=DEVICE)\n",
        "    with torch.no_grad():\n",
        "      targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "      predictions = model(data)\n",
        "      loss = loss_fn(predictions, targets)\n",
        "      losses.append(loss)\n",
        "    avg_loss = sum(losses)/len(losses)\n",
        "    loop.set_description(f\"Test\")\n",
        "    loop.set_postfix(loss = avg_loss.cpu().numpy())\n",
        "\n",
        "    torchvision.utils.save_image(\n",
        "            predictions, f\"{folder}/pred_{batch_idx}.png\"\n",
        "        )\n",
        "    torchvision.utils.save_image(targets, f\"{folder}/mask_{batch_idx}.png\")\n",
        "    torchvision.utils.save_image(data, f\"{folder}/ori_{batch_idx}.png\")\n",
        "  model.train()\n",
        "  return avg_loss, losses\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAA5t_bsSNfQ"
      },
      "source": [
        "##Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "for i in range(10):\n",
        "  torch.cuda.empty_cache()\n",
        "  import gc\n",
        "  gc.collect()\n",
        "\n",
        "# Hyperparameters etc.\n",
        "LEARNING_RATE = 1e-5\n",
        "# 1e-4 --> overfitss \"Train loss decrease while the val loss increase\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE_TRAIN = 4\n",
        "BATCH_SIZE_VAL = 1\n",
        "BATCH_SIZE_TEST = 1\n",
        "NUM_EPOCHS = 50\n",
        "NUM_WORKERS = 1\n",
        "IMAGE_HEIGHT = 680 # 1280 originally\n",
        "IMAGE_WIDTH = 680  # 1918 originally\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "import tensorflow as tf\n",
        "TRAIN_IMG_DIR = \"/content/unzip/ISIC-2017_Training_Data/\"\n",
        "TRAIN_MASK_DIR = \"/content/unzip/ISIC-2017_Training_Part1_GroundTruth/\"\n",
        "VAL_IMG_DIR = \"/content/unzip/ISIC-2017_Validation_Data/\"\n",
        "VAL_MASK_DIR = \"/content/unzip/ISIC-2017_Validation_Part1_GroundTruth/\"\n",
        "TEST_IMG_DIR = \"/content/unzip/ISIC-2017_Test_v2_Data\"\n",
        "TEST_MASK_DIR = \"/content/unzip/ISIC-2017_Test_v2_Part1_GroundTruth\"\n",
        "\n",
        "train_transform, val_transforms = get_transformations()\n",
        "\n",
        "train_loader, val_loader, test_loader = get_loaders(\n",
        "    TRAIN_IMG_DIR,\n",
        "    TRAIN_MASK_DIR,\n",
        "    VAL_IMG_DIR,\n",
        "    VAL_MASK_DIR,\n",
        "    TEST_IMG_DIR,\n",
        "    TEST_MASK_DIR,\n",
        "    BATCH_SIZE_TRAIN,\n",
        "    BATCH_SIZE_VAL,\n",
        "    BATCH_SIZE_TEST,\n",
        "    train_transform,\n",
        "    val_transforms,        \n",
        "    PIN_MEMORY,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "4O8yVs838PtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huVjHkSidJ5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    loss_tracker = 1\n",
        "\n",
        "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "       load_checkpoint(torch.load(\"/content/drive/MyDrive/Colab Notebooks/Computer Vision/my_checkpoint2.pth.tar\"), model)\n",
        "       loss_tracker = val_fn(val_loader, model, loss_fn)\n",
        "       loss_tracker = loss_tracker.cpu().numpy()\n",
        "       print(loss_tracker)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    # print(sum(i > 0.5 for i in scores))\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    #   # print(sum(scores.data) / len(scores.data))\n",
        "      \n",
        "      train_fn(train_loader, model, optimizer,loss_fn, scaler)\n",
        "\n",
        "      avg_loss = val_fn(val_loader, model, loss_fn)\n",
        "      avg_loss = avg_loss.cpu().numpy()\n",
        "      print(avg_loss)\n",
        "      if (avg_loss < loss_tracker): \n",
        "        loss_tracker = avg_loss       \n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\":optimizer.state_dict(),\n",
        "        }\n",
        "        save_checkpoint(checkpoint)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, checkpoint):  \n",
        "  load_checkpoint(torch.load(checkpoint), model)\n",
        "  scores, losses = test_fn(test_loader, model, loss_fn, \"/content/saved_images\")\n",
        "  return scores, losses"
      ],
      "metadata": {
        "id": "eUUQN8hn8MPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
        "checkpoint = \"/content/drive/MyDrive/Colab Notebooks/Computer Vision/my_checkpoint2.pth.tar\"\n",
        "scores, losses = test(model, checkpoint)\n",
        "\n",
        "print(f\"\\nAverage IOU loss: {scores}\")\n",
        "j2 = [i for i in losses if i < 0.5]\n",
        "print(f\"{round((len(j2)/len(losses))*100,2)} % of the test images with loss less than 0.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_qPUTQ77iAj",
        "outputId": "63eba6e4-182a-403b-f45b-2946886dddd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, loss=0.3036067]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average IOU loss: 0.30360668897628784\n",
            "79.33 % of the test images with loss less than 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# min_value = max(losses)\n",
        "# min_value = losses == 0.4167\n",
        "min_value = [i for i in losses if i < 0.5]\n",
        "# min_value = losses[25]\n",
        "index = 10\n",
        "min_index = losses.index(min_value[index])\n",
        "# min_index = losses.index(min_value)\n",
        "img_path_pred = \"/content/saved_images/pred_\"+str(min_index)+\".png\"\n",
        "img_path_org = \"/content/saved_images/mask_\"+str(min_index)+\".png\"\n",
        "img_pred = cv2.imread(img_path_pred)\n",
        "img_org = cv2.imread(img_path_org)\n",
        "\n",
        "\n",
        "\n",
        "print(min_value[index])\n",
        "cv2_imshow(img_pred)\n",
        "cv2_imshow(img_org)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPvvJz6173UD",
        "outputId": "7cc4f617-c6f2-47aa-d9ac-cba737259696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3598, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqgAAAKoCAIAAAA02poLAAAQfUlEQVR4nO3d34tU9f8H8Bkz2bKQMrVt7ZcpqYVdCZGVkZL9sISKJG+CpCwhBMUwvAgkJYLox40klSz4ox+IoZJWFlZsRgXphZAgQdHaWmtppZuWzfei/TafXXfdmdkz8zoz5/H4A3afc2R9nef7/T5ncjkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKps4sSJra2thULh448//vHHH7/++usvv/yypaUlOhcAkJw5c+Z89dVXhYEcOnRo8eLF0WEBgIoMGTJk1apVA8770x04cGDRokXR8QGAkm3durWCkd/L5s2boz8HAHBGt9566+BHfi+zZ8+O/lgAwGkee+yxxKf+fxYsWBD9+QCA/1e9kX+6W265JfrjAkCG1XLq/+fo0aNr166dNWtW9KcHgCwJmfq9fPPNN+vWrYu+EgDQ6Pbt2xc99Ht46qmnoi8JADSobdu2RQ/6vnV2dkZfGwBoLOvWrYue7wNobW2NvkgA0Ciix3qpNm7cGH2pAKDO3X777dEDvQzbt2+PvmAAUM+iR3klXn755ejLBtSHfHQASJcxY8Z0dHREp6hQPu8vGhjAkOgAkC7fffdddITKFQqFJUuWRKcAUk0/gB4KhUJ0hMFqaWk5ePBgdAogpTR+KFq9enV0hAS0t7cvXbo0OgUApF57e3v0Kb3EzJgxI/pyAmlkqR+Kfvjhh5aWlugUSXLcD+jFUj8UDRs2LDpCwgqFwpQpU6JTACli8EPRqFGjoiMkb+/evdu2bYtOAQDpE70vX0ULFy6MvrpAKtj/g27XXXfdnj17olNUkf1+IGepH/7z888/R0eori+++CI6AhDP4Idul1xySXSE6po6deq9994bnQIIZvBDtwZ7kK9PmzZtio4ABLPnB0WF+n9f74A6Ozsb8uEFoEQGP3Rramrq6uqKTlELTvlBlvn7h6IsNP5/mf2QWfb4IYu80gcyy+CHLLrrrrseeOCB6BRAAMt9UJSdpf5/WfCHDNL4Ibs+/PDD6AhArbnfh6KsNf6c0g/Zo/FDprW2tkZHAGrKzT4UZbDxt7e3jx07NjoFUDsaP2RaS0vL/Pnzo1MAtaPxQ1EGG/+/7PRDdmj8QG7NmjXREYAacZsPRZlt/DmlHzJD4wdyuVxuy5Yt0RGAWnCPD0VZbvy5XG748OHHjx+PTgFUl8EPRRkf/DkL/pABlvqh6KOPPoqOEGzx4sXREYDqcncPRRp/LpcbNmzYX3/9FZ0CqBaNH4o2b94cHSHe3r17oyMAVWTwQ9GRI0eiI8SbNGnSypUro1MA1WKpH4pmzJixc+fO6BSp4JQfNCqNH4q+/fbb6Ahpceedd0ZHAKrCTT0UjRw5srOzMzpFWij90JA0fiiaPHlydIQUmTZtWnQEIHnu6KEHT/T9p62t7cYbb4xOASTM4IceDP7/ZbUfGo+lfqBfK1asiI4AJMztPPSg8fei9EOD0fiBMxk5cmR0BCBJBj/08Oyzz0ZHSBfPN0KDMfihh927d0dHSJ2rrroqOgKQGLt30Jtt/tPZ6YeGofFDbwcPHoyOAFAtBj/09uCDD0ZHSB2rINAwDH7o7ZNPPomOkEZPPvlkdAQgAfbtoA+7du2aPn16dIrUsdMPDUDjhz48//zz0RHSaNGiRdERgMFy/w59s6vdJ6Uf6p3GD3179913oyOk0UsvvRQdARgUN+/QL6W/TxMnTty/f390CqBCGj9QnvHjx0dHACqn8UO/zj///N9++y06RRrZ6Yf6pfFDv37//ffoCCm1evXq6AgAUAVz5swp0JfRo0dH/+MAlbBeBwMoOOLXl59++mnMmDHRKYCyWeqHAbS1tUVHSKPRo0dPnTo1OgVQNo0fBnDTTTd5e39/nPKDuuOPFgZmtb8/+/fvnzhxYnQKoAyW+mFghw4dio6QUldfffUdd9wRnQIog8YPJVH6z8CCP9QRjR8YrMcffzw6AlAq9+lQEo3/zJR+qBcaP5Sks7MzOkKqrVixIjoCUBI36VAqpf/MmpubOzo6olMAA9D4gWRs3LgxOgIwMI0fSvXPP//YyT6zadOmffbZZ9EpgDPR+KFUy5Yti46Qdl5vDOmnvkAZbPMP6P333581a1Z0CqBfBj+UweAvxYQJEw4cOBCdAuibwQ9lMPhLcerUqaFDh0anAPpmjx9I2FlnnXXNNddEpwD6pvFDeZT+EnkCAtJJ4weqYtKkSdERgD64JYfyaPwl6ujoaG5ujk4B9KbxQ3m8n65EF198cXQEoA8GP5Rn3rx50RHqxtq1a6MjAL1Z6oeyWe0vnSN+kDYaP5Tt1KlT0RHqxpVXXhkdAejB4IeyzZw5MzpC3fCdPZA2VuGgElb7S2e1H1JF44dK7Nu3LzpC3RgxYkR0BKDI4IdKXHvttdER6saaNWuiIwBFluCgQlb7S9TV1XXuuedGpwC6GfxQoZMnT5599tnRKeqDbX5ID0v9UKH77rsvOkLd8O5eSA+DHyq0devW6Ah144knnoiOAHSz/gaVs81fOqv9kBIaP1TuiiuuiI4AUB734DAoSn+JNH5ICY0fqIVly5ZFRwByOYMfBmnLli3REerDQw89FB0ByOUs9cPgWe0vkdV+SAONHwAyxOCHwbr00kujIwCUysobJMBqfynOOeecP//8MzoFZJ3GDwnYs2dPdIQ6YOpDGhj8kIDly5dHR6gDo0aNio4AWOqHhFjtH9BFF110+PDh6BSQdRo/JGPTpk3REdLOtxhDGhj8kIz7778/OkLajRs3LjoCYPADtXLeeedFRwDs8UNybPOfWVNT04kTJ6JTQNZp/JCY9vb26AipNnv27OgIgMEPyRk7dmx0hFSz1A9pYPADNXLPPfdERwAMfqBWmpqaoiMABj8k6o033oiOkF7jx4+PjgAY/JConTt3RkdIL+/qhzTwOB8kzEN9/eno6Ghubo5OAVmn8QM14jsMIQ00fkiYxn8G+bz/cyCYxg/UyPr166MjABo/JG3v3r1TpkyJTpFGhUJhyBBlA4L5I4SEtba2RkdIqVdffTU6AmDwQ9K8mLY/N998c3QEwOCHpK1YsSI6Qkpt2LAhOgJg8AO1Mm7cuOgIgMEP1Mrq1aujIwAGP1TB9OnToyOk0eTJk6MjAB7ng+rwGp/TjRw58pdffolOAVmn8QM1ctlll0VHAAx+qI7t27dHR0gd7+qHNDD4oSp8Py+QTvb4oVps8/fiG3ogDTR+qJZZs2ZFRwDozQ04VJHS/780fkgDjR+q6O23346OANCDG3CoLqX/Pxo/pIHGD9X13HPPRUcAKHIDDlWn9OdyuY6Ojubm5ugUgMYP1Wfg5XK5HTt2REcAcjmDH2qgo6PD9vaECROiIwC5nMEPNfPWW29FR4i0bdu26AhALmfwQ83MnTt3yZIl0SnCjB49OjoCkMs53Ac1ltmDfqNGjers7IxOAWj8UFv5fP7IkSPRKQKMGDEiOgKQy2n8ECKDvd/xRkgJjR8CmIJAFIMfYuTz+VOnTkWnADLH4IcwQ4cOXb58eXSKWliwYEF0BKCbwQ+RVq1atXTp0ugUVff3339HRwC62WiEVGjs437ONEB6aPyQCvl83vf4ATXgNhzSpfGq//fff3/55ZdHpwC6afyQLvl8/s0334xOkaR58+ZFRwCAetDa2lqof9FXEejBUj/Ugba2thtuuCE6RYWc7INU8QcJ9aTuCrTv5oG0sccP9SSfz999993RKcpg6gNAAt57773ovfuBzZw5M/o6AUCjePrpp6Mn+wCirxAANJa2trbo4d6v+fPnR18eoA8O90F9K6S1WDvMD+nkcB/Ut3TO1/Xr10dHAPqWxv8ygLKksPSn83YEyGn80ADSNmV37NgRHQEAGtptt90WfZivaPLkydHXAwAaXfS47/bCCy9EXwkAyIDt27dHD/1CIX2nDQCgYUUP/cLixYujrwEAZMbcuXMDp/7x48ejLwAAZEzg4H/99dejPz0AZMyGDRuiBn/0RwdKkq7Hf4HBC5nBaXuXANAfL/CBRjN37twa/8ZHH320xr8RqJibdGhAtSz9v/7664UXXlizXwcMksYPDaiWFfy1116r2e8CAPr24osvOtMHnM5SPzSsGkxlZ/qg7ljqh4ZV7al8/fXXV/XnAwBlq9IK/8qVK6M/GQDQl8Sn/jPPPBP9mYAK2Z+DTCgkt9/f1dV1wQUXnDhxIqkfCNSSwQ+ZMHz48D/++CORH+VAH9Q1h/sgE44dO5bIwF64cOHgfwgAUCOD2dp/5513ouMDAGWqbOq/8sor0cEBgIp0dXWVPvKPHj26e/fu6MgAwCB88MEHpUz9zz///JFHHokOCwAM2uzZs3ft2tXfyD927NjDDz8cnRFImMdygNyePXs+/fTTkydPNjU1LVy4cPny5YcPH7apDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQKb8HyTpwJLpXhx7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=680x680 at 0x7F5FC02BF790>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=680x680 at 0x7F5FC275E650>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6D3I-E7n8w0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XKSHLGoddwEk",
        "H4S7NxTcZBpw",
        "Wih1bMUqbabr",
        "_ZF5JJBYdIiH",
        "DRPuyFsJH9KU",
        "O8bFPoRbPsUD"
      ],
      "machine_shape": "hm",
      "name": "UNet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}